{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batur/anaconda3/envs/fv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch, numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.FV_utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.FV_utils.intervention_utils import function_vector_intervention\n",
    "from src.FV_utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.FV_utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.FV_utils.eval_utils import decode_to_vocab, sentence_eval\n",
    "\n",
    "from src.LTV_utils.data import set_seed\n",
    "from src.LTV_utils.ltv import LearnableTaskVector\n",
    "from src.LTV_utils.extract_utils import get_attn_out\n",
    "from src.LTV_utils.intervention_utils import ltv_intervention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "GPU_IDX = 0\n",
    "SEED = 17\n",
    "\n",
    "device = torch.device(f\"cuda:{GPU_IDX}\")\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)\n",
    "EDIT_LAYER = 9\n",
    "\n",
    "n_layers = model_config['n_layers']\n",
    "resid_dim = model_config['resid_dim']\n",
    "n_heads = model_config['n_heads']\n",
    "head_dim = resid_dim // n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and compute the task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'commonsense_qa'\n",
    "act_fn = None\n",
    "loss_fn = F.cross_entropy\n",
    "batch_size = 100\n",
    "\n",
    "dataset = load_dataset(task_name, root_data_dir='dataset_files', seed=0)\n",
    "mean_activations, _ = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where could you find a toilet that anyone can use?\\na: bathroom\\nb: apartment\\nc: stall\\nd: hospital\\ne: rest area'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(task_name, root_data_dir='dataset_files', seed=5)['train']['input'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute **Function Vector (FV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained **Learnable Task Vector (LTV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltv_seq_len = 5\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "path_to_model = os.path.join(os.path.dirname(current_directory), f\"language/LTV_models/{task_name}/{act_fn}/seq_len_{ltv_seq_len}\")\n",
    "path_to_model = os.path.join(path_to_model, f\"ltv_layer_{ltv_seq_len}.pth\")\n",
    "\n",
    "ltv_layer = LearnableTaskVector(n_layers, n_heads, head_dim).to(device)\n",
    "ltv_params = torch.load(path_to_model)\n",
    "ltv_layer.load_state_dict(ltv_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt Creation:** \n",
    "#### 1. Standard In-Context\n",
    "#### 2. Shuffled-Label\n",
    "#### 3. Zero-Shot\n",
    "#### 4. Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " \"<|endoftext|>Q: What is the opposite of little?\\na: least\\nb: much\\nc: bog\\nd: lot of\\ne: big\\nA: e\\n\\nQ: When people aren't communicating or talking to each other, what happens?\\na: misunderstandings\\nb: headaches\\nc: introductions\\nd: conversation\\ne: distraction\\nA: a\\n\\nQ: What does a person who is a gardener have?\\na: own house\\nb: contribution to society\\nc: food\\nd: ride horses\\ne: green thumb\\nA: e\\n\\nQ: Where is not likely to organize with a card catalog?\\na: libary\\nb: store\\nc: kitchen\\nd: bank\\ne: library\\nA: b\\n\\nQ: What is it called when you spend time with friends and acquaintances?\\na: socialize\\nb: tell story\\nc: go somewhere\\nd: wedding\\ne: clean room\\nA: a\\n\\nQ: A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives\\nA:\" \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " \"<|endoftext|>Q: What is the opposite of little?\\na: least\\nb: much\\nc: bog\\nd: lot of\\ne: big\\nA: e\\n\\nQ: When people aren't communicating or talking to each other, what happens?\\na: misunderstandings\\nb: headaches\\nc: introductions\\nd: conversation\\ne: distraction\\nA: e\\n\\nQ: What does a person who is a gardener have?\\na: own house\\nb: contribution to society\\nc: food\\nd: ride horses\\ne: green thumb\\nA: a\\n\\nQ: Where is not likely to organize with a card catalog?\\na: libary\\nb: store\\nc: kitchen\\nd: bank\\ne: library\\nA: b\\n\\nQ: What is it called when you spend time with friends and acquaintances?\\na: socialize\\nb: tell story\\nc: go somewhere\\nd: wedding\\ne: clean room\\nA: a\\n\\nQ: A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives\\nA:\" \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "n_examples = 5\n",
    "test_idx = np.random.randint(0, len(dataset['test']))\n",
    "\n",
    "word_pairs = dataset['train'][np.random.choice(len(dataset['train']), n_examples, replace=False)]\n",
    "test_pair = dataset['test'][test_idx]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives',\n",
       " 'output': 'e'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: \"<|endoftext|>Q: What is the opposite of little?\\na: least\\nb: much\\nc: bog\\nd: lot of\\ne: big\\nA: e\\n\\nQ: When people aren't communicating or talking to each other, what happens?\\na: misunderstandings\\nb: headaches\\nc: introductions\\nd: conversation\\ne: distraction\\nA: a\\n\\nQ: What does a person who is a gardener have?\\na: own house\\nb: contribution to society\\nc: food\\nd: ride horses\\ne: green thumb\\nA: e\\n\\nQ: Where is not likely to organize with a card catalog?\\na: libary\\nb: store\\nc: kitchen\\nd: bank\\ne: library\\nA: b\\n\\nQ: What is it called when you spend time with friends and acquaintances?\\na: socialize\\nb: tell story\\nc: go somewhere\\nd: wedding\\ne: clean room\\nA: a\\n\\nQ: A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives\\nA:\" \n",
      "\n",
      "Input Query: 'A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives'\n",
      "Target: 'e'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' e', 0.26383), (' a', 0.20441), (' d', 0.19414), (' c', 0.1753), (' b', 0.14085)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}\\nTarget: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn_out = get_attn_out(mean_activations, model, model_config)\n",
    "    lt_vector = ltv_layer.forward(attn_out)\n",
    "    lt_vector = lt_vector.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffled-Label Few-Shot ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: \"<|endoftext|>Q: What is the opposite of little?\\na: least\\nb: much\\nc: bog\\nd: lot of\\ne: big\\nA: e\\n\\nQ: When people aren't communicating or talking to each other, what happens?\\na: misunderstandings\\nb: headaches\\nc: introductions\\nd: conversation\\ne: distraction\\nA: e\\n\\nQ: What does a person who is a gardener have?\\na: own house\\nb: contribution to society\\nc: food\\nd: ride horses\\ne: green thumb\\nA: a\\n\\nQ: Where is not likely to organize with a card catalog?\\na: libary\\nb: store\\nc: kitchen\\nd: bank\\ne: library\\nA: b\\n\\nQ: What is it called when you spend time with friends and acquaintances?\\na: socialize\\nb: tell story\\nc: go somewhere\\nd: wedding\\ne: clean room\\nA: a\\n\\nQ: A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives\\nA:\" \n",
      "\n",
      "Input Query: 'A person misses his aunt, what is he likely to do about it?\\na: cross street\\nb: amount to nothing\\nc: seek no help\\nd: doctor himself\\ne: visit relatives'\n",
      "Target: 'e'\n",
      "\n",
      "Few-Shot-Shuffled Prompt Top k Vocab Probs:\n",
      "\t[(' a', 0.22226), (' e', 0.21166), (' d', 0.18582), (' c', 0.1844), (' b', 0.16358)]\n",
      "\n",
      "Shuffled Prompt +FV Top k Vocab Probs:\n",
      "\t[(' d', 0.2781), (' c', 0.20724), (' a', 0.17368), (' e', 0.17321), (' b', 0.15306)]\n",
      "\n",
      "Shuffled Prompt +LTV Top k Vocab Probs:\n",
      "\t[(' e', 0.20592), (' b', 0.20154), (' a', 0.19683), (' d', 0.17884), (' c', 0.17381)]\n",
      "\n",
      "Vanilla transformer - loss: 1.5528\n",
      "Function Vector - loss: 1.7532\n",
      "Learnable Task Vector - loss: 1.5803\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "_, interv_logits_ltv = ltv_intervention(shuffled_sentence, [test_pair['output']], lt_vector, model, model_config, tokenizer)\n",
    "                                                   \n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}\\nTarget: {repr(test_pair['output'])}\\n\")\n",
    "\n",
    "print(f\"Few-Shot-Shuffled Prompt Top k Vocab Probs:\\n\\t{decode_to_vocab(clean_logits, tokenizer, k=5)}\\n\")\n",
    "print(f\"Shuffled Prompt +FV Top k Vocab Probs:\\n\\t{decode_to_vocab(interv_logits_fv, tokenizer, k=5)}\\n\")\n",
    "print(f\"Shuffled Prompt +LTV Top k Vocab Probs:\\n\\t{decode_to_vocab(interv_logits_ltv, tokenizer, k=5)}\\n\")\n",
    "\n",
    "target_idx = tokenizer.encode(\" \" + test_pair['output'])\n",
    "target_idx = torch.tensor(target_idx, dtype=torch.int64).to(device)\n",
    "\n",
    "clean_loss = loss_fn(clean_logits, target_idx)\n",
    "fv_loss = loss_fn(interv_logits_fv, target_idx)\n",
    "ltv_loss = loss_fn(interv_logits_ltv, target_idx)\n",
    "\n",
    "print(f\"Vanilla transformer - loss: {clean_loss:.4f}\")\n",
    "print(f\"Function Vector - loss: {fv_loss:.4f}\")\n",
    "print(f\"Learnable Task Vector - loss: {ltv_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: Where is microphone boom likely to be used to record an album?\\na: radio station\\nb: recording studio\\nc: concert\\nd: tv studio\\ne: new york\\nA:' \n",
      "\n",
      "Input Query: 'Where is microphone boom likely to be used to record an album?\\na: radio station\\nb: recording studio\\nc: concert\\nd: tv studio\\ne: new york'\n",
      "Target: 'b'\n",
      "\n",
      "Zero-Shot Top k Vocab Probs:\n",
      "\t[(' a', 0.14372), (' b', 0.10578), (' c', 0.08973), (' e', 0.05843), (' d', 0.05265)]\n",
      "\n",
      "Zero-Shot +FV Vocab Top k Vocab Probs:\n",
      "\t[(' d', 0.21565), (' c', 0.20987), (' e', 0.14113), (' a', 0.13808), (' b', 0.13053)]\n",
      "\n",
      "Zero-Shot +LTV Vocab Top k Vocab Probs:\n",
      "\t[(' b', 0.14828), (' a', 0.1258), (' d', 0.09221), (' c', 0.06607), ('\\n', 0.03976)]\n",
      "\n",
      "Vanilla transformer - loss: 2.2464\n",
      "Function Vector - loss: 2.0361\n",
      "Learnable Task Vector - loss: 1.9086\n"
     ]
    }
   ],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "clean_output, interv_logits_ltv = ltv_intervention(zeroshot_sentence, [test_pair['output']], lt_vector, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}\\nTarget: {repr(test_pair['output'])}\\n\")\n",
    "\n",
    "print(f\"Zero-Shot Top k Vocab Probs:\\n\\t{decode_to_vocab(clean_logits, tokenizer, k=5)}\\n\")\n",
    "print(f\"Zero-Shot +FV Vocab Top k Vocab Probs:\\n\\t{decode_to_vocab(interv_logits_fv, tokenizer, k=5)}\\n\")\n",
    "print(f\"Zero-Shot +LTV Vocab Top k Vocab Probs:\\n\\t{decode_to_vocab(interv_logits_ltv, tokenizer, k=5)}\\n\")\n",
    "\n",
    "target_idx = tokenizer.encode(\" \" + test_pair['output'])\n",
    "target_idx = torch.tensor(target_idx, dtype=torch.int64).to(device)\n",
    "\n",
    "clean_loss = loss_fn(clean_logits, target_idx)\n",
    "fv_loss = loss_fn(interv_logits_fv, target_idx)\n",
    "ltv_loss = loss_fn(interv_logits_ltv, target_idx)\n",
    "\n",
    "print(f\"Vanilla transformer - loss: {clean_loss:.4f}\")\n",
    "print(f\"Function Vector - loss: {fv_loss:.4f}\")\n",
    "print(f\"Learnable Task Vector - loss: {ltv_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
